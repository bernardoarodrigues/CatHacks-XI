{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import flappy_bird_gymnasium\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# ----- World Model -----\n",
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, state_dim=12, action_dim=1, hidden_dim=128):\n",
    "        super(WorldModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, state_dim + 1)  # next_state + reward\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# ----- Train World Model -----\n",
    "def train_world_model(model, data, epochs=100, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        total_loss = 0\n",
    "        for (s, a, next_s, r) in data:\n",
    "            s_tensor = torch.tensor(s, dtype=torch.float32).unsqueeze(0)\n",
    "            a_tensor = torch.tensor([a], dtype=torch.float32).unsqueeze(0)\n",
    "            next_s = np.append(next_s, r)\n",
    "            target = torch.tensor(next_s, dtype=torch.float32).unsqueeze(0)\n",
    "            output = model(s_tensor, a_tensor)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# ----- Save & Load -----\n",
    "def save_model(model, path=\"saved_policies/world_model.pth\"):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, path=\"saved_policies/world_model.pth\"):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model\n",
    "\n",
    "# ----- Simulated Controller Example -----\n",
    "def simulate(model, start_state, steps=10):\n",
    "    state = torch.tensor(start_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    done = False\n",
    "    for _ in range(steps):\n",
    "        action = torch.tensor([[0.0]])  # Try both 0.0 and 1.0 for exploration\n",
    "        prediction = model(state, action)\n",
    "        next_state = prediction[0, :-1].detach().numpy()\n",
    "        reward = prediction[0, -1].item()\n",
    "        print(f\"Simulated Reward: {reward:.3f}\")\n",
    "\n",
    "        if reward < -1: \n",
    "            done = True\n",
    "            break\n",
    "\n",
    "        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    return next_state, reward, done, False, {}\n",
    "\n",
    "\n",
    "def select_best_action(model, current_state):\n",
    "    model.eval()\n",
    "    state_tensor = torch.tensor(current_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    best_action = None\n",
    "    best_score = -float('inf')\n",
    "\n",
    "    for action_val in [0.0, 1.0]:  # Explore both actions\n",
    "        action_tensor = torch.tensor([[action_val]], dtype=torch.float32)\n",
    "        prediction = model(state_tensor, action_tensor)\n",
    "        next_state_pred = prediction[0, :-1].detach().numpy()\n",
    "        reward_pred = prediction[0, -1].item()\n",
    "\n",
    "        # Simple scoring: reward only (can be replaced with a value function)\n",
    "        score = reward_pred  \n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_action = int(action_val)\n",
    "\n",
    "    return best_action, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Virtual World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1829\n",
      "0\n",
      "Epoch 0, Loss: 33.3325\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Epoch 10, Loss: 13.4931\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "Epoch 20, Loss: 11.3329\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "Epoch 30, Loss: 9.8330\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "Epoch 40, Loss: 9.1914\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "Model saved to saved_policies/world_model.pth\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FlappyBird-v0\", render_mode=None, use_lidar=False)\n",
    "\n",
    "sample_size = 100_0\n",
    "\n",
    "dummy_data = []\n",
    "\n",
    "\n",
    "total_reward = 0\n",
    "for i in range(sample_size):\n",
    "    done = False\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        dummy_data.append((obs, action, next_obs, reward))\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Sample {i}, Avg reward (past 100): {total_reward/100:.2f}\")\n",
    "        total_reward = 0\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(len(dummy_data))\n",
    "\n",
    "model = WorldModel()\n",
    "train_world_model(model, dummy_data, epochs=50)\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the bot play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from saved_policies/world_model.pth\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FlappyBird-v0\", render_mode=\"human\", use_lidar=False)\n",
    "\n",
    "# Load and simulate\n",
    "loaded_model = WorldModel()\n",
    "model = load_model(loaded_model)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Use the trained model\n",
    "obs, _ = env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    action, score = select_best_action(model, obs)\n",
    "    # print(action)\n",
    "\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    # print(f\"Action {action} gave {reward} reward...\")\n",
    "    # print(obs, reward, done, truncated, info)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

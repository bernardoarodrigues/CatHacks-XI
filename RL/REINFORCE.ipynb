{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‘RE’ward ‘I’ncrement ‘N’on-negative ‘F’actor times ‘O’ffset ‘R’einforcement times ‘C’haracteristic ‘E’ligibility (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "import gymnasium as gym\n",
    "import flappy_bird_gymnasium\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Network(nn.Module):\n",
    "    \"\"\"Parametrized Policy Network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initializes a neural network that estimates the mean and standard deviation\n",
    "         of a normal distribution from which an action is sampled from.\n",
    "\n",
    "        Args:\n",
    "            obs_space_dims: Dimension of the observation space\n",
    "            action_space_dims: Dimension of the action space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_space1 = 16  # Nothing special with 16, feel free to change\n",
    "        hidden_space2 = 32  # Nothing special with 32, feel free to change\n",
    "\n",
    "        # Shared Network\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(obs_space_dims, hidden_space1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_space1, hidden_space2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        # Policy Mean specific Linear Layer\n",
    "        self.policy_mean_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2, action_space_dims)\n",
    "        )\n",
    "\n",
    "        # Policy Std Dev specific Linear Layer\n",
    "        self.policy_stddev_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2, action_space_dims)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Conditioned on the observation, returns the mean and standard deviation\n",
    "         of a normal distribution from which an action is sampled from.\n",
    "\n",
    "        Args:\n",
    "            x: Observation from the environment\n",
    "\n",
    "        Returns:\n",
    "            action_means: predicted mean of the normal distribution\n",
    "            action_stddevs: predicted standard deviation of the normal distribution\n",
    "        \"\"\"\n",
    "        shared_features = self.shared_net(x.float())\n",
    "\n",
    "        action_means = self.policy_mean_net(shared_features)\n",
    "        action_stddevs = torch.log(\n",
    "            1 + torch.exp(self.policy_stddev_net(shared_features))\n",
    "        )\n",
    "\n",
    "        return action_means, action_stddevs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    \"\"\"REINFORCE algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initializes an agent that learns a policy via REINFORCE algorithm [1]\n",
    "        to solve the task at hand (Inverted Pendulum v4).\n",
    "\n",
    "        Args:\n",
    "            obs_space_dims: Dimension of the observation space\n",
    "            action_space_dims: Dimension of the action space\n",
    "        \"\"\"\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 1e-4  # Learning rate for policy optimization\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.eps = 1e-6  # small number for mathematical stability\n",
    "\n",
    "        self.probs = []  # Stores probability values of the sampled action\n",
    "        self.rewards = []  # Stores the corresponding rewards\n",
    "\n",
    "        self.net = Policy_Network(obs_space_dims, action_space_dims)\n",
    "        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def sample_action(self, state: np.ndarray) -> float:\n",
    "        \"\"\"Returns an action, conditioned on the policy and observation.\n",
    "\n",
    "        Args:\n",
    "            state: Observation from the environment\n",
    "\n",
    "        Returns:\n",
    "            action: Action to be performed\n",
    "        \"\"\"\n",
    "        state = torch.tensor(np.array([state]))\n",
    "        action_means, action_stddevs = self.net(state)\n",
    "\n",
    "        # create a normal distribution from the predicted\n",
    "        #   mean and standard deviation and sample an action\n",
    "        distrib = Normal(action_means[0] + self.eps, action_stddevs[0] + self.eps)\n",
    "        action = distrib.sample()\n",
    "        prob = distrib.log_prob(action)\n",
    "\n",
    "        action = action.numpy()\n",
    "\n",
    "        self.probs.append(prob)\n",
    "\n",
    "        return 1 if prob.item() > 0 else 0\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Updates the policy network's weights.\"\"\"\n",
    "        running_g = 0\n",
    "        gs = []\n",
    "\n",
    "        # Discounted return (backwards) - [::-1] will return an array in reverse\n",
    "        for R in self.rewards[::-1]:\n",
    "            running_g = R + self.gamma * running_g\n",
    "            gs.insert(0, running_g)\n",
    "\n",
    "        deltas = torch.tensor(gs)\n",
    "\n",
    "        log_probs = torch.stack(self.probs)\n",
    "\n",
    "        # Calculate the mean of log probabilities for all actions in the episode\n",
    "        log_prob_mean = log_probs.mean()\n",
    "\n",
    "        # Update the loss with the mean log probability and deltas\n",
    "        # Now, we compute the correct total loss by taking the sum of the element-wise products.\n",
    "        loss = -torch.sum(log_prob_mean * deltas)\n",
    "\n",
    "        # Update the policy network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Empty / zero out all episode-centric/related variables\n",
    "        self.probs = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Average Reward: 2\n",
      "Episode: 1000 Average Reward: 2\n",
      "Episode: 2000 Average Reward: 2\n",
      "Episode: 3000 Average Reward: 2\n",
      "Episode: 4000 Average Reward: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [02:25<09:43, 145.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Average Reward: 2\n",
      "Episode: 1000 Average Reward: 2\n",
      "Episode: 2000 Average Reward: 2\n",
      "Episode: 3000 Average Reward: 2\n",
      "Episode: 4000 Average Reward: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [04:40<06:57, 139.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Average Reward: 2\n",
      "Episode: 1000 Average Reward: 2\n",
      "Episode: 2000 Average Reward: 2\n",
      "Episode: 3000 Average Reward: 2\n",
      "Episode: 4000 Average Reward: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [07:05<04:43, 141.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Average Reward: 2\n",
      "Episode: 1000 Average Reward: 2\n",
      "Episode: 2000 Average Reward: 2\n",
      "Episode: 3000 Average Reward: 2\n",
      "Episode: 4000 Average Reward: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [10:09<02:38, 158.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Average Reward: 2\n",
      "Episode: 1000 Average Reward: 2\n",
      "Episode: 2000 Average Reward: 2\n",
      "Episode: 3000 Average Reward: 2\n",
      "Episode: 4000 Average Reward: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [13:20<00:00, 160.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "total_num_episodes = int(5e3)  # Total number of episodes\n",
    "obs_space_dims = 12\n",
    "action_space_dims = 1\n",
    "\n",
    "\n",
    "env = gym.make(\"FlappyBird-v0\", render_mode=None, use_lidar=False)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=total_num_episodes)\n",
    "\n",
    "\n",
    "rewards_over_seeds = []\n",
    "\n",
    "for seed in tqdm([1, 2, 3, 5, 8]):  # Fibonacci seeds\n",
    "    # set seed\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Reinitialize agent every seed\n",
    "    agent = REINFORCE(obs_space_dims, action_space_dims)\n",
    "    reward_over_episodes = []\n",
    "\n",
    "    for episode in range(total_num_episodes):\n",
    "        # gymnasium v26 requires users to set seed while resetting the environment\n",
    "        obs, info = env.reset(seed=seed)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.sample_action(obs)\n",
    "            # print(\"SAMPLED \", action)\n",
    "\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            agent.rewards.append(reward)\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "        reward_over_episodes.append(env.return_queue[-1])\n",
    "        agent.update()\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            avg_reward = int(np.mean(env.return_queue))\n",
    "            print(\"Episode:\", episode, \"Average Reward:\", avg_reward)\n",
    "\n",
    "    rewards_over_seeds.append(reward_over_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
